{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/computer-needs-glasses/blob/master/image-multimodal-models/PaliGemma_FirstContact.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using PaliGemma\n",
        "\n",
        "PaliGemma is a new vision language model released by Google."
      ],
      "metadata": {
        "id": "igGrdR8hu1Yd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zchyD7YIsmi7"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U accelerate bitsandbytes git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PaliGemma requires users to accept Gemma license, so make sure to go to [the repository]() and ask for access. If you have previously accepted Gemma license, you will have access to this model as well. Once you have the access, login to Hugging Face Hub using `notebook_login()` and pass your access token by running the cell below."
      ],
      "metadata": {
        "id": "gbF8uBFFvWFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor\n",
        "from huggingface_hub import notebook_login\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests"
      ],
      "metadata": {
        "id": "cVZoJn4aUZIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "j_NIM_Qgs5mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Who is this person and what is he famous for?\"\n",
        "img_url = \"https://huggingface.co/datasets/ritwikraha/random-storage/resolve/main/frenchman.png\"\n",
        "input_image = Image.open(requests.get(img_url, stream=True).raw)"
      ],
      "metadata": {
        "id": "YyCf7CW7vuU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can load PaliGemma model and processor like below."
      ],
      "metadata": {
        "id": "kwwhOe6A2gHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_id = \"google/paligemma-3b-mix-224\"\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
        "processor = PaliGemmaProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "HKYYuWe_s7Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processor preprocesses both the image and text, so we will pass them."
      ],
      "metadata": {
        "id": "Z2s9B89ekJ2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(text=input_text, images=input_image,\n",
        "                  padding=\"longest\", do_convert_rgb=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "model.to(device)\n",
        "inputs = inputs.to(dtype=model.dtype)"
      ],
      "metadata": {
        "id": "u9kau5IOjNt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can pass in our preprocessed inputs."
      ],
      "metadata": {
        "id": "cvJTWZX0jM3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  output = model.generate(**inputs, max_length=496)\n",
        "\n",
        "print(processor.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "bNRrmPcQyEfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model in 4-bit\n",
        "\n",
        "You can also load model in 4-bit and 8-bit, which offers memory gains during inference.\n",
        "First, initialize the `BitsAndBytesConfig`."
      ],
      "metadata": {
        "id": "7wdFjmxPyUsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n"
      ],
      "metadata": {
        "id": "cb9NEdq2s-nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now reload the model but pass in above object as `quantization_config`."
      ],
      "metadata": {
        "id": "TtbAZsn64VV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor\n",
        "import torch\n",
        "\n",
        "device=\"cuda\"\n",
        "model_id = \"google/paligemma-3b-mix-224\"\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,\n",
        "                                                          quantization_config=nf4_config, device_map={\"\":0})\n",
        "processor = PaliGemmaProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "-iShp-9ntAV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  output = model.generate(**inputs, max_length=496)\n",
        "\n",
        "print(processor.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "A7GSnVVUy8lJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}