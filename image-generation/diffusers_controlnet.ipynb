{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9iHmwN198ABDhLT+FTq4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/computer-needs-glasses/blob/master/image-generation/diffusers_controlnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Imports"
      ],
      "metadata": {
        "id": "GHucTpcVeg18"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeL762hXbgtL"
      },
      "outputs": [],
      "source": [
        "!pip intsall diffusers accelerate xformers opencv-contrib-python controlnet_aux -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from transformers import pipeline, AutoImageProcessor, UperNetForSemanticSegmentation\n",
        "from diffusers.utils import load_image\n",
        "from diffusers import UniPCMultistepScheduler, StableDiffusionControlNetPipeline, ControlNetModel\n",
        "from controlnet_aux import OpenposeDetector, HEDdetector"
      ],
      "metadata": {
        "id": "iTccdvg4byrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Gt3uPrrzbpSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images and other Assets"
      ],
      "metadata": {
        "id": "crP4uG6Fej1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = load_image(\n",
        "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
        ")\n",
        "image"
      ],
      "metadata": {
        "id": "7BsZCuMxcTv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "_7X_kYahenbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n"
      ],
      "metadata": {
        "id": "koB3fDRndIFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_canny_image(image, low_threshold=100, high_threshold=200):\n",
        "    \"\"\"Converts an image to its Canny edge representation.\n",
        "\n",
        "    Args:\n",
        "        image: A PIL Image or a NumPy array representing the image to process.\n",
        "        low_threshold: The lower threshold for the hysteresis procedure. (default: 100)\n",
        "        high_threshold: The upper threshold for the hysteresis procedure. (default: 200)\n",
        "\n",
        "    Returns:\n",
        "        A PIL Image containing the Canny edges of the input image.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = np.array(image)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Ensure grayscale conversion\n",
        "    canny_edges = cv2.Canny(image, low_threshold, high_threshold)\n",
        "\n",
        "    # Replicate the single-channel edges to create a 3-channel image for display\n",
        "    canny_image = Image.fromarray(np.stack((canny_edges,) * 3, axis=-1))\n",
        "\n",
        "    return canny_image"
      ],
      "metadata": {
        "id": "5dyGvKVIcVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Canny ControlNet"
      ],
      "metadata": {
        "id": "Iv82M0jbgOob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
        ")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "9tW8Z3pqcihK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \", best quality, extremely detailed\"\n",
        "prompt = [t + prompt for t in [\"Sandra Oh\", \"Kim Kardashian\", \"rihanna\", \"taylor swift\"]]\n",
        "generator = [torch.Generator(device=\"cpu\").manual_seed(2) for i in range(len(prompt))]"
      ],
      "metadata": {
        "id": "vzpMXFs4cvT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(\n",
        "    prompt,\n",
        "    canny_image=create_canny_image(image),\n",
        "    negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * 4,\n",
        "    num_inference_steps=20,\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "image_grid(output.images, 2, 2)"
      ],
      "metadata": {
        "id": "HYushnw8dkBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pose ControlNet"
      ],
      "metadata": {
        "id": "5kECodCKgUL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = \"yoga1.jpeg\", \"yoga2.jpeg\", \"yoga3.jpeg\", \"yoga4.jpeg\"\n",
        "imgs = [\n",
        "    load_image(\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/\" + url)\n",
        "    for url in urls\n",
        "]\n",
        "\n",
        "image_grid(imgs, 2, 2)"
      ],
      "metadata": {
        "id": "1z2I36bXdriS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "\n",
        "poses = [model(img) for img in imgs]\n",
        "image_grid(poses, 2, 2)"
      ],
      "metadata": {
        "id": "DUc-1qc3dtzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusing two ControlNet Models\n"
      ],
      "metadata": {
        "id": "RBIJf-exgXFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"fusing/stable-diffusion-v1-5-controlnet-openpose\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "qrDEiXlld0Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "canny_image = create_canny_image(image)\n",
        "# canny_image = np.array(canny_image)\n",
        "\n",
        "# zero out middle columns of image where pose will be overlayed\n",
        "zero_start = canny_image.shape[1] // 4\n",
        "zero_end = zero_start + canny_image.shape[1] // 2\n",
        "canny_image[:, zero_start:zero_end] = 0\n",
        "\n",
        "canny_image = canny_image[:, :, None]\n",
        "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
        "canny_image = Image.fromarray(canny_image)"
      ],
      "metadata": {
        "id": "BnQ9kI8gd-K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "\n",
        "openpose_image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n",
        ")\n",
        "openpose_image = openpose(openpose_image)\n"
      ],
      "metadata": {
        "id": "tyWVjgfDeL4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "controlnet = [\n",
        "    ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16),\n",
        "    ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16),\n",
        "]\n",
        "\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
        ")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "pipe.enable_model_cpu_offload()"
      ],
      "metadata": {
        "id": "4lu3R6RdeNw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a giant standing in a fantasy landscape, best quality\"\n",
        "negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
        "\n",
        "generator = torch.Generator(device=\"cpu\").manual_seed(1)\n",
        "\n",
        "images = [openpose_image, canny_image]\n",
        "\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    images,\n",
        "    num_inference_steps=20,\n",
        "    generator=generator,\n",
        "    negative_prompt=negative_prompt,\n",
        "    controlnet_conditioning_scale=[1.0, 0.8],\n",
        ").images[0]"
      ],
      "metadata": {
        "id": "kNCGDKemeV47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ControlNet HED (Scribble)"
      ],
      "metadata": {
        "id": "nzL5ziTli54u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\n",
        "\n",
        "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png\")\n",
        "\n",
        "image = hed(image, scribble=True)\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-scribble\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "image = pipe(\"bag\", image, num_inference_steps=20).images[0]\n"
      ],
      "metadata": {
        "id": "ILRa9hfHgb4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ControlNet Depth"
      ],
      "metadata": {
        "id": "hSli9yiGi3Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depth_estimator = pipeline('depth-estimation')\n",
        "\n",
        "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\n",
        "\n",
        "image = depth_estimator(image)['depth']\n",
        "image = np.array(image)\n",
        "image = image[:, :, None]\n",
        "image = np.concatenate([image, image, image], axis=2)\n",
        "image = Image.fromarray(image)\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-depth\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "image = pipe(\"Stormtrooper's lecture\", image, num_inference_steps=20).images[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "hTbLrmHChXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ControlNet Segmentation"
      ],
      "metadata": {
        "id": "W5IMrWK_ilJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palette = np.asarray([\n",
        "    [0, 0, 0],\n",
        "    [120, 120, 120],\n",
        "    [180, 120, 120],\n",
        "    [6, 230, 230],\n",
        "    [80, 50, 50],\n",
        "    [4, 200, 3],\n",
        "    [120, 120, 80],\n",
        "    [140, 140, 140],\n",
        "    [204, 5, 255],\n",
        "    [230, 230, 230],\n",
        "    [4, 250, 7],\n",
        "    [224, 5, 255],\n",
        "    [235, 255, 7],\n",
        "    [150, 5, 61],\n",
        "    [120, 120, 70],\n",
        "    [8, 255, 51],\n",
        "    [255, 6, 82],\n",
        "    [143, 255, 140],\n",
        "    [204, 255, 4],\n",
        "    [255, 51, 7],\n",
        "    [204, 70, 3],\n",
        "    [0, 102, 200],\n",
        "    [61, 230, 250],\n",
        "    [255, 6, 51],\n",
        "    [11, 102, 255],\n",
        "    [255, 7, 71],\n",
        "    [255, 9, 224],\n",
        "    [9, 7, 230],\n",
        "    [220, 220, 220],\n",
        "    [255, 9, 92],\n",
        "    [112, 9, 255],\n",
        "    [8, 255, 214],\n",
        "    [7, 255, 224],\n",
        "    [255, 184, 6],\n",
        "    [10, 255, 71],\n",
        "    [255, 41, 10],\n",
        "    [7, 255, 255],\n",
        "    [224, 255, 8],\n",
        "    [102, 8, 255],\n",
        "    [255, 61, 6],\n",
        "    [255, 194, 7],\n",
        "    [255, 122, 8],\n",
        "    [0, 255, 20],\n",
        "    [255, 8, 41],\n",
        "    [255, 5, 153],\n",
        "    [6, 51, 255],\n",
        "    [235, 12, 255],\n",
        "    [160, 150, 20],\n",
        "    [0, 163, 255],\n",
        "    [140, 140, 140],\n",
        "    [250, 10, 15],\n",
        "    [20, 255, 0],\n",
        "    [31, 255, 0],\n",
        "    [255, 31, 0],\n",
        "    [255, 224, 0],\n",
        "    [153, 255, 0],\n",
        "    [0, 0, 255],\n",
        "    [255, 71, 0],\n",
        "    [0, 235, 255],\n",
        "    [0, 173, 255],\n",
        "    [31, 0, 255],\n",
        "    [11, 200, 200],\n",
        "    [255, 82, 0],\n",
        "    [0, 255, 245],\n",
        "    [0, 61, 255],\n",
        "    [0, 255, 112],\n",
        "    [0, 255, 133],\n",
        "    [255, 0, 0],\n",
        "    [255, 163, 0],\n",
        "    [255, 102, 0],\n",
        "    [194, 255, 0],\n",
        "    [0, 143, 255],\n",
        "    [51, 255, 0],\n",
        "    [0, 82, 255],\n",
        "    [0, 255, 41],\n",
        "    [0, 255, 173],\n",
        "    [10, 0, 255],\n",
        "    [173, 255, 0],\n",
        "    [0, 255, 153],\n",
        "    [255, 92, 0],\n",
        "    [255, 0, 255],\n",
        "    [255, 0, 245],\n",
        "    [255, 0, 102],\n",
        "    [255, 173, 0],\n",
        "    [255, 0, 20],\n",
        "    [255, 184, 184],\n",
        "    [0, 31, 255],\n",
        "    [0, 255, 61],\n",
        "    [0, 71, 255],\n",
        "    [255, 0, 204],\n",
        "    [0, 255, 194],\n",
        "    [0, 255, 82],\n",
        "    [0, 10, 255],\n",
        "    [0, 112, 255],\n",
        "    [51, 0, 255],\n",
        "    [0, 194, 255],\n",
        "    [0, 122, 255],\n",
        "    [0, 255, 163],\n",
        "    [255, 153, 0],\n",
        "    [0, 255, 10],\n",
        "    [255, 112, 0],\n",
        "    [143, 255, 0],\n",
        "    [82, 0, 255],\n",
        "    [163, 255, 0],\n",
        "    [255, 235, 0],\n",
        "    [8, 184, 170],\n",
        "    [133, 0, 255],\n",
        "    [0, 255, 92],\n",
        "    [184, 0, 255],\n",
        "    [255, 0, 31],\n",
        "    [0, 184, 255],\n",
        "    [0, 214, 255],\n",
        "    [255, 0, 112],\n",
        "    [92, 255, 0],\n",
        "    [0, 224, 255],\n",
        "    [112, 224, 255],\n",
        "    [70, 184, 160],\n",
        "    [163, 0, 255],\n",
        "    [153, 0, 255],\n",
        "    [71, 255, 0],\n",
        "    [255, 0, 163],\n",
        "    [255, 204, 0],\n",
        "    [255, 0, 143],\n",
        "    [0, 255, 235],\n",
        "    [133, 255, 0],\n",
        "    [255, 0, 235],\n",
        "    [245, 0, 255],\n",
        "    [255, 0, 122],\n",
        "    [255, 245, 0],\n",
        "    [10, 190, 212],\n",
        "    [214, 255, 0],\n",
        "    [0, 204, 255],\n",
        "    [20, 0, 255],\n",
        "    [255, 255, 0],\n",
        "    [0, 153, 255],\n",
        "    [0, 41, 255],\n",
        "    [0, 255, 204],\n",
        "    [41, 0, 255],\n",
        "    [41, 255, 0],\n",
        "    [173, 0, 255],\n",
        "    [0, 245, 255],\n",
        "    [71, 0, 255],\n",
        "    [122, 0, 255],\n",
        "    [0, 255, 184],\n",
        "    [0, 92, 255],\n",
        "    [184, 255, 0],\n",
        "    [0, 133, 255],\n",
        "    [255, 214, 0],\n",
        "    [25, 194, 194],\n",
        "    [102, 255, 0],\n",
        "    [92, 0, 255],\n",
        "])\n"
      ],
      "metadata": {
        "id": "asiiF60OiL_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "\n",
        "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-seg/resolve/main/images/house.png\").convert('RGB')\n",
        "\n",
        "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = image_segmentor(pixel_values)\n",
        "\n",
        "seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
        "\n",
        "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "\n",
        "for label, color in enumerate(palette):\n",
        "    color_seg[seg == label, :] = color\n",
        "\n",
        "color_seg = color_seg.astype(np.uint8)\n",
        "\n",
        "image = Image.fromarray(color_seg)\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-seg\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "image = pipe(\"house\", image, num_inference_steps=20).images[0]"
      ],
      "metadata": {
        "id": "Z07nMCXriQib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}