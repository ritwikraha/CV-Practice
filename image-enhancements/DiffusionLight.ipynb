{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/CV-Practice/blob/master/DiffusionLight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlcSADvrgYEx"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers -qq\n",
        "!pip install accelerate -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXYc0oaZfwUR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers.utils import load_image\n",
        "from diffusers import StableDiffusionXLControlNetInpaintPipeline, ControlNetModel\n",
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTdIG5mkl8hM"
      },
      "outputs": [],
      "source": [
        "# load pipeline\n",
        "# Load the ControlNet model, which is part of the inpainting pipeline.\n",
        "# This model is loaded with specific configurations.\n",
        "controlnet = ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4sTb9zsuQMS"
      },
      "outputs": [],
      "source": [
        "# Load the Stable Diffusion XL Control Net Inpainting Pipeline with the previously loaded ControlNet model.\n",
        "# This pipeline is used for image inpainting tasks.\n",
        "pipe = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        ")  # Moving the pipeline to CUDA for GPU acceleration.\n",
        "\n",
        "pipe.enable_model_cpu_offload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WAxYM-jggVJ"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "IS_UNDER_EXPOSURE = False # A boolean variable used as a flag. If set to True, it indicates that the output should be underexposed.\n",
        "\n",
        "# Conditional statement to check the value of IS_UNDER_EXPOSURE.\n",
        "if IS_UNDER_EXPOSURE:\n",
        "    # If IS_UNDER_EXPOSURE is True, set the PROMPT variable to a string describing a black, dark, mirrored, reflective chrome ball.\n",
        "    PROMPT = \"a perfect black dark mirrored reflective chrome ball sphere\"\n",
        "else:\n",
        "    # If IS_UNDER_EXPOSURE is False, set the PROMPT variable to a string describing a mirrored, reflective chrome ball without the underexposure characteristics.\n",
        "    PROMPT = \"a perfect mirrored reflective chrome ball sphere\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VobepqcZhbtq"
      },
      "source": [
        "Photo by <a href=\"https://unsplash.com/@loewe?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Loewe Technology</a> on <a href=\"https://unsplash.com/photos/a-living-room-with-a-large-book-shelf-and-a-television-u9ar6U_o5oU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVKbVn9fg1sn"
      },
      "outputs": [],
      "source": [
        "NEGATIVE_PROMPT = \"matte, diffuse, flat, dull\"\n",
        "IMAGE_URL = \"https://i.imgur.com/0FwdO10.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dtOQK2hLgQRU"
      },
      "outputs": [],
      "source": [
        "# Load LoRA weights into the pipeline.\n",
        "pipe.load_lora_weights(\"DiffusionLight/DiffusionLight\")\n",
        "\n",
        "# Fuse LoRA layers into the model with a specified scaling factor.\n",
        "# LoRA layers are used to adjust the model's behavior without extensive retraining.\n",
        "pipe.fuse_lora(lora_scale=0.75)\n",
        "\n",
        "# Load a depth estimation pipeline.\n",
        "# This model is used to estimate the depth of objects in images, which is useful in various computer vision tasks.\n",
        "depth_estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
        "\n",
        "# prepare input image\n",
        "# Load an image from a given URL. This image will be used as the input for the depth estimation model.\n",
        "init_image = load_image(IMAGE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_image.resize((1024,1024))"
      ],
      "metadata": {
        "id": "ntibqEJElavn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the depth estimation model to the loaded image to generate a depth image.\n",
        "# This depth image represents the perceived depth of objects in the input image.\n",
        "depth_image = depth_estimator(images=init_image)['depth']"
      ],
      "metadata": {
        "id": "6fwUvDkolZJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDqebI8RgJ6A"
      },
      "outputs": [],
      "source": [
        "# Function definition for creating a circular mask.\n",
        "def get_circle_mask(size=256):\n",
        "    # Create a linear space from -1 to 1 with 'size' number of elements.\n",
        "    x = torch.linspace(-1, 1, size)\n",
        "    y = torch.linspace(1, -1, size)\n",
        "\n",
        "    # Create a 2D grid using the x and y tensors.\n",
        "    y, x = torch.meshgrid(y, x)\n",
        "\n",
        "    # Compute the z values to form a circle. Points inside the circle have z >= 0.\n",
        "    z = (1 - x**2 - y**2)\n",
        "\n",
        "    # Create a mask where the values inside the circle are True (1) and outside are False (0).\n",
        "    mask = z >= 0\n",
        "    return mask\n",
        "\n",
        "# Generate the circular mask.\n",
        "mask = get_circle_mask().numpy()\n",
        "\n",
        "# Convert the depth image to a numpy array.\n",
        "depth = np.asarray(depth_image).copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuVZ8o0uit7I"
      },
      "outputs": [],
      "source": [
        "# Apply the mask to a specific region of the depth image.\n",
        "# The region from (384, 384) to (640, 640) is modified.\n",
        "#### NOTE: You can choose your own region or take it as user input ####\n",
        "# The depth values inside the circle are set to 255 (maximum depth).\n",
        "depth[384:640, 384:640] = depth[384:640, 384:640] * (1 - mask) + (mask * 255)\n",
        "\n",
        "# Convert the modified depth array back to an image.\n",
        "depth_mask = Image.fromarray(depth)\n",
        "\n",
        "# Create an image to represent the mask itself. Initialize with zeros (black image).\n",
        "mask_image = np.zeros_like(depth)\n",
        "\n",
        "# Apply the circular mask to the same region as before. The mask values are set to 255 (white) inside the circle.\n",
        "mask_image[384:640, 384:640] = mask * 255\n",
        "\n",
        "# Convert the mask array back to an image.\n",
        "mask_image = Image.fromarray(mask_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK63ntwEgkMS"
      },
      "outputs": [],
      "source": [
        "# run the pipeline\n",
        "output = pipe(\n",
        "    prompt=PROMPT,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    num_inference_steps=30,\n",
        "    image=init_image,\n",
        "    mask_image=mask_image,\n",
        "    control_image=depth_mask,\n",
        "    controlnet_conditioning_scale=0.5,\n",
        ")\n",
        "\n",
        "# save output\n",
        "output[\"images\"][0].save(\"output.png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZAhXLqvj1UZhk5k8E8Zob",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}